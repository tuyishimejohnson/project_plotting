{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ],
      "metadata": {
        "id": "xyATLU4z1cYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary package\n",
        "import numpy as np\n",
        "\n",
        "import numpy.linalg as eig\n"
      ],
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "**[Scaling your data in machine learning (ML)is important because many algorithms use the Euclidean distance between two data points in their\n",
        "computations/derivations, which is sensitive to the scale of the variables. If one variable is on a much larger scale than another, that variable will\n",
        "dominate the distance calculation, and the algorithm will be affected by that variable more than the other. (Wei Hao Khoong, 2023)]**"
      ],
      "metadata": {
        "id": "U2U2_Q5ebos3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mean = np.mean(data, axis=0) # mean of each columnd\n",
        "standard_deviation =  np.std(data, axis=0) # this is the standard deviaation of each column.\n",
        "standardized_data = (data - mean) /standard_deviation # calculating the satandardized data\n",
        "print(standardized_data)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JF3eGB7FRC0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f6b03e-e061-468f-9b07-f60dc79e0e59"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.36438208  0.70710678  1.5109662  -0.99186978  0.77802924]\n",
            " [ 0.12403473 -1.94454365 -0.13736056  0.77145428 -2.06841919]\n",
            " [-0.62017367  0.1767767   0.68680282 -0.99186978  0.20873955]\n",
            " [ 1.61245155  0.1767767  -1.78568733  0.33062326  0.20873955]\n",
            " [-0.62017367  1.23743687 -0.13736056 -0.77145428  1.00574511]\n",
            " [ 0.86824314 -0.35355339 -0.13736056  1.65311631 -0.13283426]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ],
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "uuhux3UEcBgw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8oujZlK9YR",
        "outputId": "bcd0496f-04f0-49b1-99ff-0df700e00742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2.16666667  -1.06666667  -1.76666667   5.5         -4.36666667]\n",
            " [ -1.06666667   4.26666667   0.46666667  -6.6         19.66666667]\n",
            " [ -1.76666667   0.46666667   1.76666667  -3.3          2.36666667]\n",
            " [  5.5         -6.6         -3.3         24.7        -27.9       ]\n",
            " [ -4.36666667  19.66666667   2.36666667 -27.9         92.56666667]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cov_matrix = np.cov(data, rowvar=False) # the covariance of data\n",
        "print(cov_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n"
      ],
      "metadata": {
        "id": "uXNcG4AFcT08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "eigenvalues, eigenvectors =np.linalg.eig(cov_matrix) # eigenvalues, eigenvectors\n",
        "print(\"EValues:\", eigenvalues)\n",
        "print(\"EVectors:\", eigenvalues)"
      ],
      "metadata": {
        "id": "dmGlQ47tRO5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118f2d1b-f71e-4ad7-fa4d-ccf057bc1609"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EValues: [1.07224751e+02 1.61823788e+01 1.93173735e+00 1.27579741e-01\n",
            " 2.20003762e-04]\n",
            "EVectors: [1.07224751e+02 1.61823788e+01 1.93173735e+00 1.27579741e-01\n",
            " 2.20003762e-04]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ],
      "metadata": {
        "id": "4pWho88fcbJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list\n",
        "\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1] # reversing the eigen values by slicing\n",
        "print ( 'the order of importance is :\\n {}'.format(order_of_importance))\n",
        "\n",
        "# utilize the sort order to sort eigenvalues and eigenvectors\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "\n",
        "print('\\n\\n sorted eigen values:\\n{}'.format(sorted_eigenvalues))\n",
        "sorted_eigenvectors = eigenvectors[:, order_of_importance] # sort the columns\n",
        "print('\\n\\n The sorted eigen vector matrix is: \\n {}'.format(sorted_eigenvectors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "75b4e95f-a0ce-4deb-d0e3-a24b8febeb71"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the order of importance is :\n",
            " [0 1 2 3 4]\n",
            "\n",
            "\n",
            " sorted eigen values:\n",
            "[1.07224751e+02 1.61823788e+01 1.93173735e+00 1.27579741e-01\n",
            " 2.20003762e-04]\n",
            "\n",
            "\n",
            " The sorted eigen vector matrix is: \n",
            " [[-0.05817655 -0.2631212   0.57237125  0.6292347  -0.45148374]\n",
            " [ 0.19774895 -0.03283879  0.06849106 -0.60720902 -0.7657827 ]\n",
            " [ 0.0328828   0.17887983 -0.75671562  0.45776292 -0.42983171]\n",
            " [-0.33200499 -0.88598416 -0.30234056 -0.11461168  0.01609676]\n",
            " [ 0.91989252 -0.33574235 -0.06059523  0.11259736  0.15724145]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "Ordering eigen values and eigen vectors helps in  Data visualization because when reducing data to two or three dimensions, ordering the eigenvalues helps in selecting the most informative dimensions.\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "In Principal Component Analysis eigenvalues that are highest are considered first.\n"
      ],
      "metadata": {
        "id": "o1nILNGxpTJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "BWqFGNeNvgEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "\n",
        "sum_of_eigenvalues = np.sum(sorted_eigenvalues) # sum of eigen values\n",
        "explained_variance = (sorted_eigenvalues / sum_of_eigenvalues) * 100 # calculating the variance\n",
        "explained_variance =[\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "print( explained_variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "e598d27b-5065-47ec-906d-c208235b594c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['85.46%', '12.90%', '1.54%', '0.10%', '0.00%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ],
      "metadata": {
        "id": "qB7H4InbfKx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 2 # select the number of principal components\n",
        "\n",
        "reduced_data = sorted_eigenvectors[:, order_of_importance[:k]]\n",
        "reduced_data = np.matmul(standardized_data, reduced_data) # transform the original data\n",
        "print(reduced_data)"
      ],
      "metadata": {
        "id": "C-Rnyq6QVTiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d9547b0-00e9-44d4-9f05-d55cda38a688"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.31389845  1.22362226]\n",
            " [-2.55511419  0.01760889]\n",
            " [ 0.61494463  1.08892909]\n",
            " [-0.03531847 -1.11250845]\n",
            " [ 1.45756867  0.44379893]\n",
            " [-0.7959791  -1.66145072]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "a0303529-127d-4ba3-93e8-1bbe7686abde"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.31389845  1.22362226]\n",
            " [-2.55511419  0.01760889]\n",
            " [ 0.61494463  1.08892909]\n",
            " [-0.03531847 -1.11250845]\n",
            " [ 1.45756867  0.44379893]\n",
            " [-0.7959791  -1.66145072]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "3d1e87f0-6721-459e-8eaa-d3a44e4b8d45"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "Give 2 Benefits and 2 limitations\n",
        "\n",
        "Benefits\n",
        "\n",
        "Reducing Dimensionality: PCA can lower a dataset's feature count while keeping the most crucial data. This can assist remove noise or unnecessary characteristics from the data and make further analysis easier and faster.\n",
        "\n",
        "Visualizing high-dimensional data in a 2D or 3D environment through principle Component Analysis (PCA) can aid in understanding the structure and relationships within the data by decreasing dimensionality to two or three principle components.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Limitations\n",
        "\n",
        "PCA assumes there is a linear underlying structure to the data. Thus, if the data has a nonlinear structure, it might not function well. Moreover, principle component alignment is another assumption made by PCA, which is not actually true.\n",
        "\n",
        "Loss of Interpretability: The physical meaning of the major components has changed because they are now just linear combinations of the original features. The interpretation of PCA results in relation to the original characteristics may become challenging as a result.\n"
      ],
      "metadata": {
        "id": "UxQ8lTunauMQ"
      }
    }
  ]
}